{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWkrLF3sA3pS"
      },
      "outputs": [],
      "source": [
        "#anqi wu\n",
        "#12.08.2024\n",
        "\n",
        "#idea on tf-idf\n",
        "#idea on fine-tuning bert on 900 labeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file preparation\n",
        "#TF-IDF computation file only with labeled dataset\n",
        "#this can be find in merged-data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# load merged data\n",
        "merged_data_path = '/content/merged_data.csv'  # please locate this in our shared folder\n",
        "merged_data = pd.read_csv(merged_data_path)\n",
        "\n",
        "label_column = 'label_x'\n",
        "drop_columns = ['url', 'label_x']\n",
        "\n",
        "# #debug\n",
        "# if labeled_data.empty:\n",
        "#     raise ValueError(\"No labeled data available for training.\")\n",
        "\n",
        "#training features\n",
        "features = labeled_data.drop(columns=drop_columns).select_dtypes(include=['float64', 'int64']).fillna(0)\n",
        "labels = labeled_data[label_column]\n",
        "\n",
        "#split and train\n",
        "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#random forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "#validation\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n"
      ],
      "metadata": {
        "id": "be4qwZWYBd0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "oeks4JMNCdYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gradient boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "TfQ-2fLzCoOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "WojFWbJMCrUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kmean\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "JYmz753xC4pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "model = MLPClassifier(random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "P5NGlLH3C8iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "#pretained on bert\n",
        "#gpu\n",
        "#make sure installation\n",
        "#pip install transformers datasets torch pandas scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "# load\n",
        "file_path = 'label_tokenized.xlsx' #refer this in our shared folder #file name should be this\n",
        "data = pd.read_excel(file_path)\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "Rm1ypgVLDE1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only put high accuracy one\n",
        "#please refer notebook for other pretained model\n",
        "from sklearn.utils import resample\n",
        "\n",
        "#oversampling minority approach\n",
        "majority_class = data[data['label'] == 2]  # Negative\n",
        "minority_class_1 = data[data['label'] == 0]  # No Attitude\n",
        "minority_class_2 = data[data['label'] == 1]  # Positive\n",
        "oversampled_class_1 = resample(minority_class_1, replace=True, n_samples=len(majority_class), random_state=42)\n",
        "oversampled_class_2 = resample(minority_class_2, replace=True, n_samples=len(majority_class), random_state=42)\n",
        "\n",
        "balanced_data = pd.concat([majority_class, oversampled_class_1, oversampled_class_2])\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(balanced_data['label'].value_counts())"
      ],
      "metadata": {
        "id": "VEMZSpcjDcUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = balanced_data\n",
        "\n",
        "#refer from tutorial and hw4\n",
        "from transformers import BertTokenizer\n",
        "from datasets import Dataset\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# create dataset dictionary from oversampled data\n",
        "data_dict = {\n",
        "    \"text\": balanced_data['processed_body'].tolist(),\n",
        "    \"label\": balanced_data['label'].tolist()\n",
        "}\n",
        "\n",
        "# convert to Hugging Face\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "# tokenization function\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# split into train and test datasets\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "nsGcVYwjD9ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,  #epoch was increased from 3 to 5, could be overfitting\n",
        "    weight_decay=0.01,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        ")"
      ],
      "metadata": {
        "id": "bp4oA1ceEPSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer\n",
        "import torch\n",
        "\n",
        "# initialization\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "\n",
        "#class weighted differently\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(balanced_data['label']),\n",
        "    y=balanced_data['label']\n",
        ")\n",
        "\n",
        "# class weights to tensor\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "#customer weighted labels\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
        "        loss = loss_fn(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "psXLeyXOEcqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# prediction\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "#validation report\n",
        "print(classification_report(true_labels, predicted_labels, target_names=[\"No Attitude\", \"Positive\", \"Negative\"]))\n"
      ],
      "metadata": {
        "id": "ZhACWAQhEspW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}