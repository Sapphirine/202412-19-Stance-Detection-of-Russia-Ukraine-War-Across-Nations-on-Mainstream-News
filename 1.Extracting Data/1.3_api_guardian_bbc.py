# -*- coding: utf-8 -*-
"""api_guardian_bbc

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sjMzQfnTSbO5eSSI2xatQ9TFMUd9wEVv
"""

#anqi wu
#eecs 6893
#extracting guardian news articles
#group 19

##try if guardian api allow me to reuqest

#guardian api
#try out session

import requests
url = 'https://content.guardianapis.com/search'
params = {
    'q': 'russia', #russia related
    'from-date': '2022-02-20',
    'to-date': '2024-11-15',
    'show-fields': 'all',
    'api-key': '2de114be-10d5-438d-bc6b-7f83f4aea1e9'
}

response = requests.get(url, params=params)
data = response.json()

import json

with open('guardian_results.json', 'w') as file:
    json.dump(data, file, indent=4)
print("Results saved to guardian_results.json")
#saved as a json file

#exploring the data structure
articles = data['response']['results']
articles

#create a table to save them in db
import sqlite3
conn = sqlite3.connect('guardian_articles.db')
cursor = conn.cursor()

cursor.execute('''
    CREATE TABLE IF NOT EXISTS articles (
        id TEXT PRIMARY KEY,
        title TEXT,
        url TEXT,
        published_date TEXT,
        body TEXT
    )
    ''')

for article in articles:
    cursor.execute('''
        INSERT OR IGNORE INTO articles (id, title, url, published_date, body)
        VALUES (?, ?, ?, ?, ?)
        ''', (
        article['id'],
        article['webTitle'],
        article['webUrl'],
        article['webPublicationDate'],
        article['fields'].get('body', 'No body available')
    ))

conn.commit()
conn.close()

print("Articles saved to SQLite database 'guardian_articles.db'")

#look at the head
conn = sqlite3.connect('guardian_articles.db')
cursor = conn.cursor()
cursor.execute('SELECT * FROM articles LIMIT 5')
rows = cursor.fetchall()

for row in rows:
    print(row)

conn.close()

##actual pulling russia news from guardian

#create function to fetch and add to the new
import requests
import sqlite3
import json

#key and parameter
url = 'https://content.guardianapis.com/search'
api_key = '2de114be-10d5-438d-bc6b-7f83f4aea1e9'

#sql set up
conn = sqlite3.connect('guardian_articles.db')
cursor = conn.cursor()

#empty table
cursor.execute('''
    CREATE TABLE IF NOT EXISTS articles (
        id TEXT PRIMARY KEY,
        title TEXT,
        url TEXT,
        published_date TEXT,
        body TEXT
    )
''')

# function to fetch articles
def fetch_articles(page=1):
    '''
    the function allow me to fetch article in guardian api
    with desired query
    args:
      page (int): page number to fetch articles from, the default set is 1
    returns:
      dict: the JSON response from the guardian api
      None: return error
    '''
    params = {
        'q': 'russia',
        'from-date': '2022-02-20',
        'to-date': '2024-11-15',
        'show-fields': 'all',
        'api-key': api_key,
        'page': page,
        'page-size': 50
    }
    #notes
    #query is russia
    #date is from 2022-2-20 to 2024-11-15

    response = requests.get(url, params=params)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: {response.status_code}")
        return None

# function to save articles to the database
def save_articles(articles):
    '''
    the function saves a list of articles into the SQLite database
    args:
      articles (list): A list of article dictionaries retrieved from the Guardian API. Each dictionary
      should contain the following keys:
        - 'id': A unique identifier for the article.
        - 'webTitle': The title of the article.
        - 'webUrl': The URL of the article.
        - 'webPublicationDate': The publication date of the article.
        - 'fields': A dictionary containing additional fields, including 'body'/if body exist
    returns:
      None
    '''
    for article in articles: #dummy type setup for
        cursor.execute('''
            INSERT OR IGNORE INTO articles (id, title, url, published_date, body)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            article['id'],
            article['webTitle'],
            article['webUrl'],
            article['webPublicationDate'],
            article['fields'].get('body', 'No body available')
        ))
    conn.commit()

#fetch from page 1
page = 1
while True:
    print(f"Fetching page {page}...")
    data = fetch_articles(page)
    if data:
        articles = data['response']['results']
        if not articles:
            print("No more articles to fetch.")
            break
        save_articles(articles)
        page += 1
    else:
        print("Stopping due to an error or no more data.")
        break


#checking
cursor.execute("SELECT COUNT(*) FROM articles")
row_count = cursor.fetchone()[0]
print(f"Total rows in the database: {row_count}") #14920

cursor.execute("SELECT * FROM articles LIMIT 5")
rows = cursor.fetchall()
for row in rows:
    print(row)

conn.close()

#check head
# with just title and published date
conn = sqlite3.connect('guardian_articles.db')
cursor = conn.cursor()
cursor.execute('SELECT title, published_date FROM articles LIMIT 10')
rows = cursor.fetchall()
for row in rows:
    print(row)
conn.close()

##for fetching data in ukraine

import requests
import sqlite3
from bs4 import BeautifulSoup
import time

# API and database configurations
url = 'https://content.guardianapis.com/search'
api_key = '2de114be-10d5-438d-bc6b-7f83f4aea1e9'
db_path = 'guardian_articles.db'

def fetch_articles(query, page=1):
    '''
    Function to fetch articles from the Guardian API for a specific query.
    Args:
        query (str): The search term for articles.
        page (int): Page number to fetch articles from, default is 1.
    Returns:
        dict: The JSON response from the Guardian API.
        None: If an error occurs or no response is received.
    '''
    params = {
        'q': query,
        'from-date': '2022-02-20',
        'to-date': '2024-11-15',
        'show-fields': 'all',
        'api-key': api_key,
        'page': page,
        'page-size': 50
    }

    response = requests.get(url, params=params)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: {response.status_code}")
        return None

def save_articles(articles):
    '''
    Function to save a list of articles into the SQLite database with duplicate URL checking and HTML parsing.
    Args:
        articles (list): A list of article dictionaries retrieved from the Guardian API.
    Returns:
        None
    '''
    conn = sqlite3.connect(db_path, timeout=10)
    cursor = conn.cursor()

    rows = []
    for article in articles:
        cursor.execute('SELECT 1 FROM guardian_articles WHERE url = ?', (article['webUrl'],))
        if cursor.fetchone():
            print(f"Skipping duplicate article: {article['webUrl']}")
            continue
        body = article['fields'].get('body', 'No body available')
        if body:
            soup = BeautifulSoup(body, 'html.parser')
            clean_text = soup.get_text(strip=True)
        else:
            clean_text = 'No body available'
        rows.append((
            article['id'],
            article['webTitle'],
            article['webUrl'],
            article['webPublicationDate'],
            clean_text
        ))

    cursor.executemany('''
        INSERT OR IGNORE INTO guardian_articles (id, title, url, published_date, body)
        VALUES (?, ?, ?, ?, ?)
    ''', rows)

    conn.commit()
    conn.close()

def fetch_and_save(query):
    '''
    Function to fetch articles for a given query and save them into the SQLite database.
    Args:
        query (str): Search query term.
    Returns:
        None
    '''
    conn = sqlite3.connect(db_path, timeout=10)
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS guardian_articles (
            id TEXT PRIMARY KEY,
            title TEXT,
            url TEXT,
            published_date TEXT,
            body TEXT
        )
    ''')

    cursor.execute('PRAGMA journal_mode=WAL;')
    conn.commit()
    conn.close()

    page = 1
    while True:
        print(f"Fetching page {page} for query '{query}'...")
        data = fetch_articles(query, page)
        if data:
            articles = data['response']['results']
            if not articles:
                print("No more articles to fetch.")
                break
            save_articles(articles)  # Save each batch of articles
            page += 1
        else:
            print("Stopping due to an error or no more data.")
            break

fetch_and_save('ukraine')

conn = sqlite3.connect('guardian_articles.db')
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM guardian_articles")
row_count = cursor.fetchone()[0]
print(f"Total rows in the database: {row_count}")

##for bbc

import pandas as pd

#the original one has entertainment information, we want to exclude those podcast and video and mainly focused on only news article
#so we filtered the original source
file_path = "bbc.csv"
df = pd.read_csv(file_path)

news_keywords = ["/news/", "/articles/", "/world/", "/politics/", "/economy/", "/business/", "/science/", "/technology/", "/health/"]
exclude_keywords = ["/iplayer/", "/sounds/", "/sport/", "/entertainment/", "/education/"]

df_filtered = df[df['url'].str.contains('|'.join(news_keywords)) & ~df['url'].str.contains('|'.join(exclude_keywords))]

filtered_file_path = "filtered_bbc.csv"
df_filtered.to_csv(filtered_file_path, index=False)

print(f"Filtered {len(df_filtered)} news-related articles.")
print(f"Filtered data saved to {filtered_file_path}.")
df_filtered #9800

import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

filtered_file_path = "filtered_bbc.csv"
df_filtered = pd.read_csv(filtered_file_path)

def get_article_body(url):
    """
    Fetches and extracts the article body from a BBC news URL.
    Args:
        url (str): The URL of the article.
    Returns:
        str: The text content of the article body or an error message if scraping fails.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        article_body_parts = []
        content_classes = [
            "ssrcss-uf6wea-RichTextComponentWrapper", #check both
            "ssrcss-1q0x1qg-Paragraph",  # common for paragraphs
        ]

        for class_name in content_classes:
            elements = soup.find_all("div", {"class": class_name})
            if elements:
                for element in elements:
                    text = element.get_text(separator=" ").strip()
                    if text:
                        article_body_parts.append(text)

        if article_body_parts:
            return " ".join(article_body_parts)

        fallback_text = soup.get_text(separator=" ").strip()
        if fallback_text:
            return fallback_text

        return "No article body found."
    except Exception as e:
        return f"Error fetching body: {str(e)}"


tqdm.pandas()
df_filtered["body"] = df_filtered["url"].progress_apply(get_article_body)

updated_file_path = "bbc_with_body.csv"
df_filtered.to_csv(updated_file_path, index=False)
print(f"Updated data with article bodies saved to {updated_file_path}.")

df_filtered

from sqlalchemy import create_engine

# Replace this with your DataFrame
# df_filtered = your DataFrame

# Database connection setup (adjust the path for your database)
db_path = '/content/drive/MyDrive/eecs6893_groupproject/eecs6893_data/guardian_articles.db'
engine = create_engine(f'sqlite:///{db_path}')

# Write DataFrame to the database
df_filtered.to_sql('df_filtered', con=engine, if_exists='replace', index=False)

print("DataFrame 'df_filtered' has been successfully transferred to the database as a table.")

##combination then union for unique information
#for guardian

from sqlalchemy import create_engine
import pandas as pd

# Connect to the SQLite database
db_path = '/content/drive/MyDrive/eecs6893_groupproject/eecs6893_data/guardian_articles.db'
engine = create_engine(f'sqlite:///{db_path}')

# Combine the tables with a UNION query
union_query = """
SELECT
    id AS source_id,
    title AS source_title,
    url AS source_url,
    published_date AS source_published_date,
    body AS source_body,
    'articles' AS source_table
FROM articles
UNION
SELECT
    id AS source_id,
    title AS source_title,
    url AS source_url,
    published_date AS source_published_date,
    body AS source_body,
    'guardian_articles' AS source_table
FROM guardian_articles;
"""
guardian_final_df = pd.read_sql(union_query, con=engine)
guardian_final_df.to_sql('guardian_final', con=engine, if_exists='replace', index=False)

print("The combined table 'guardian_final' has been created in the database.")

#############bbc_preview################
# Save as a CSV file
bbc_preview.to_csv("bbc_preview.csv", index=False)

# Reload the DataFrame
bbc_preview = pd.read_csv("bbc_preview.csv")

##########for future reference###########