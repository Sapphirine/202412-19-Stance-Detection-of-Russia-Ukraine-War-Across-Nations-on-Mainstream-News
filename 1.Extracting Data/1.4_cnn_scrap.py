# -*- coding: utf-8 -*-
"""CNN_Euronews_Scrap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKxcQ71WVkuPDFk1nC1GOVoKMbznMegf
"""

#Zishun Shen
#modified by Anqi Wu

####cnn web scraping source code

# # prompt: A sample code of using Rapid API to scrap from CNN.

# import requests

# url = "https://cnn-api1.p.rapidapi.com/category"

# querystring = {"q": "covid", "page": "1", "size": "10"}

# headers = {
# 	"X-RapidAPI-Key": "a87224e81cmsh841aa37a5b5418cp1eff42jsnfb4271ce7e3d", # Replace with your actual API key
# 	"X-RapidAPI-Host": "cnn-api1.p.rapidapi.com"
# }

# response = requests.request("GET", url, headers=headers, params=querystring)

# response.text

!pip install requests beautifulsoup4 tqdm

import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import tqdm
from google.colab import drive
import time

# # Mount Google Drive
# drive.mount('/content/drive')

def extract_article_text(url):
  """
    extracts the text content of an article from the given URL.
    args:
        url (str): The URL of the article to extract text from.

    returns:
        Optional[str]: The extracted article text if successful, otherwise None.
    """
  try:
      response = requests.get(url)
      response.raise_for_status()

      soup = BeautifulSoup(response.content, 'html.parser')

      # extract article body based on <p></p>
      article_text = ""
      for p in soup.find_all('p'):
          article_text += p.get_text() + "\n"
      return article_text.strip()

  except requests.exceptions.RequestException as e:
      print(f"Error fetching URL {url}: {e}")
      return ""

# each file path
# !ls '/content/drive/MyDrive/BDA'
CNN_ukraine = "ukraine-cnn-11_15.xlsx"
CNN_russia = "russia-cnn-11_15.xlsx"
db_CNN = "cnn_articles.db"

# Read data from Excel files
df_cnn_UA = pd.read_excel(CNN_ukraine)[["language", "media_name", "publish_date", "title", "url"]] # ukraine-cnn-11/15

# filter out links to videos
# we focused on articles text
df_cnn_UA = df_cnn_UA[~df_cnn_UA["url"].str.contains("/video")]

#after exploration
#only the URLs in the format "https://www.cnn.com/202X" because I believe that only this kind of url redirects to a single article
df_cnn_UA = df_cnn_UA[df_cnn_UA["url"].str.startswith("https://www.cnn.com/202")]

# russia-cnn-11/15
df_cnn_RU = pd.read_excel(CNN_russia)[["language", "media_name", "publish_date", "title", "url"]]
df_cnn_RU = df_cnn_RU[~df_cnn_RU["url"].str.contains("/video")]
df_cnn_RU = df_cnn_RU[df_cnn_RU["url"].str.startswith("https://www.cnn.com/202")]

df_cnn = pd.concat([df_cnn_UA, df_cnn_RU], ignore_index=True)
df_cnn.drop_duplicates(subset="url", inplace=True)
df_cnn["body"] = None
df_cnn

# create database and table
conn = sqlite3.connect(db_CNN)
cursor = conn.cursor()

cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='cnn';")
table_exists = cursor.fetchone()

if table_exists:
    pass
else:
    df_cnn.to_sql('cnn', conn, index=False)

for index, row in tqdm.tqdm(df_cnn.iterrows(), total=df_cnn.shape[0], desc="Scraping articles"):
    url = row['url']
    cursor = conn.execute("SELECT body FROM cnn WHERE url = ? AND (body IS NULL)", (url,))
    result = cursor.fetchone()

    if result:
        body = extract_article_text(url)
        conn.execute("UPDATE cnn SET body = ? WHERE url = ?", (body, url))
        conn.commit()
        time.sleep(1) # Add a delay to avoid overwhelming the server

conn.close()
print("Scraping completed and database updated.")

# df_cnn['body'] = df_cnn['url'].apply(get_article_text)

# df_cnn.to_csv('/content/drive/MyDrive/BDA/cnn_articles-11_15.csv', index=False)

# df_cnn.head()

# path to save the CSV file
csv_CNN = "cnn_articles.csv"
conn = sqlite3.connect(db_CNN)
cursor = conn.cursor()
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()

all_data = pd.DataFrame()
for table_name in tables:
    table_name = table_name[0]
    df = pd.read_sql_query(f"SELECT * FROM {table_name};", conn)
    all_data = pd.concat([all_data, df], ignore_index=True)

#log
all_data.to_csv(csv_CNN, index=False)
print(f"All data exported to {csv_CNN}")
conn.close()
print("Export completed!")

all_data

df_cnn_UA_1 = pd.read_excel(CNN_ukraine)[["language", "media_name", "publish_date", "title", "url"]] # ukraine-cnn-11/15
df_cnn_UA_1 = df_cnn_UA_1[~df_cnn_UA_1["url"].str.contains("/video")]
df_cnn_UA_1 = df_cnn_UA_1[~df_cnn_UA_1["url"].str.startswith("https://www.cnn.com/202")]
df_cnn_UA_1.head()
